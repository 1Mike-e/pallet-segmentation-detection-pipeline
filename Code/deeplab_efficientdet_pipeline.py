# -*- coding: utf-8 -*-
"""DL project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HCj2XR3_eagsWWKTS7K4nSL5Riorh28Q

##Connecting to drive
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p /content/drive/MyDrive/pallet_project
# %cd /content

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/AksheyaKS/PeerRoboticsAssessment.git repo
# %cd repo
!git checkout -b alt-arch-Mike

"""##Insatllations"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip -q install segmentation-models-pytorch==0.3.3 albumentations==1.4.3 opencv-python==4.10.0.84
!pip -q install timm==1.0.7
!pip -q install effdet==0.4.1 pycocotools==2.0.7 onnx onnxruntime-gpu==1.18.1

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall -y numpy onnxruntime onnx opencv-python pycocotools

# Commented out IPython magic to ensure Python compatibility.
# %pip install "numpy==1.26.4"

# Commented out IPython magic to ensure Python compatibility.
# %pip install "opencv-python==4.10.0.84" "pycocotools==2.0.7"

# Commented out IPython magic to ensure Python compatibility.
# %pip install "onnx==1.16.2" "onnxruntime-gpu==1.17.1"
# (If you don't have a GPU in Colab, use onnxruntime==1.17.1 instead)

# Commented out IPython magic to ensure Python compatibility.
# %pip install "albumentations==1.4.3" "segmentation-models-pytorch==0.3.3" "effdet==0.4.1" "timm==1.0.7"
# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

"""##Verify Installations"""

import numpy, cv2, albumentations, torch
import onnxruntime as ort, onnx
print("numpy", numpy.__version__)            # 1.26.4
print("opencv", cv2.__version__)
print("albumentations", albumentations.__version__)
print("torch", torch.__version__)
print("onnx", onnx.__version__)              # 1.16.2
print("onnxruntime", ort.__version__)        # 1.17.1

"""##Dataset Preparation and Augmenatation"""

# imports
import os, glob, cv2, numpy as np, torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A

root = "/content/drive/MyDrive/pallet_project/EuroPalletSeg"

train_imgs = sorted(
    glob.glob(f"{root}/train/images/*.jpg")
  + glob.glob(f"{root}/train/images/*.jpeg")
  + glob.glob(f"{root}/train/images/*.png")
)
val_imgs = sorted(
    glob.glob(f"{root}/valid/images/*.jpg")
  + glob.glob(f"{root}/valid/images/*.jpeg")
  + glob.glob(f"{root}/valid/images/*.png")
)

print("paths:", len(train_imgs), len(val_imgs))

class SegDataset(Dataset):
    def __init__(self, img_paths, root, split, size=512):
        self.img_paths = list(img_paths)
        self.root = root
        self.split = split
        self.size = size
        mean = (0.485, 0.456, 0.406)
        std  = (0.229, 0.224, 0.225)
        self.augs = A.Compose([
            A.LongestMaxSize(max_size=size),
            A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.3),
            A.Normalize(mean=mean, std=std, max_pixel_value=255.0),
        ])

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, i):
        ip = str(self.img_paths[i])
        fn = os.path.splitext(os.path.basename(ip))[0]
        mp = os.path.join(self.root, self.split, "masks", f"{fn}.png")

        img_bgr = cv2.imread(ip)
        assert img_bgr is not None, f"Can't read image: {ip}"
        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

        msk = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)
        assert msk is not None, f"Missing mask: {mp}"
        msk = (msk > 127).astype(np.uint8)

        aug = self.augs(image=img, mask=msk)
        img, msk = aug["image"], aug["mask"]

        img = np.transpose(img.astype(np.float32), (2,0,1))
        msk = msk[np.newaxis, ...].astype(np.float32)
        return torch.from_numpy(img), torch.from_numpy(msk)


train_ds = SegDataset(train_imgs, root, "train", size=512)
val_ds   = SegDataset(val_imgs,   root, "valid", size=512)

print("lens:", len(train_ds), len(val_ds))


train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)
val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0)

import os, glob

root = "/content/drive/MyDrive/pallet_project/EuroPalletSeg"

print("train images:", len(glob.glob(f"{root}/train/images/*")))
print("train masks :", len(glob.glob(f"{root}/train/masks/*")))
print("valid images:", len(glob.glob(f"{root}/valid/images/*")))
print("valid masks :", len(glob.glob(f"{root}/valid/masks/*")))

print("sample train image:", os.path.basename(glob.glob(f"{root}/train/images/*")[0]))
print("sample train mask :", os.path.basename(glob.glob(f"{root}/train/masks/*")[0]))

from pathlib import Path
import os, glob

root = "/content/drive/MyDrive/pallet_project/EuroPalletSeg"

def find_mask_for_image(img_path: str, split: str):
    p = Path(img_path)
    stem = p.stem
    mask_dir = Path(root) / split / "masks"
    # try common candidates
    for name in (f"{stem}.png", f"{stem}.jpg", f"{stem}.jpeg",
                 f"{stem}_mask.png", f"{stem}_mask.jpg", f"{stem}_mask.jpeg"):
        mp = mask_dir / name
        if mp.exists():
            return str(mp)
    return None

# Build paired lists (only keep images that have a found mask)
def build_pairs(split: str):
    imgs = sorted(
        glob.glob(f"{root}/{split}/images/*.jpg") +
        glob.glob(f"{root}/{split}/images/*.jpeg") +
        glob.glob(f"{root}/{split}/images/*.png")
    )
    pairs = []
    missing = 0
    for ip in imgs:
        mp = find_mask_for_image(ip, split)
        if mp is not None:
            pairs.append((ip, mp))
        else:
            missing += 1
    print(f"{split}: images={len(imgs)}  paired={len(pairs)}  missing_masks={missing}")
    return pairs

train_pairs = build_pairs("train")
valid_pairs = build_pairs("valid")

import cv2, numpy as np, torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A

class SegDataset(Dataset):
    def __init__(self, pairs, size=512):
        self.pairs = list(pairs)     # list of (img_path, mask_path)
        self.size = size
        self.augs = A.Compose([
            A.LongestMaxSize(max_size=size),
            A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.3),
            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225), max_pixel_value=255.0),
        ])

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, i):
        ip, mp = self.pairs[i]
        img_bgr = cv2.imread(ip);  assert img_bgr is not None, f"Can't read {ip}"
        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        msk = cv2.imread(mp, cv2.IMREAD_GRAYSCALE);  assert msk is not None, f"Can't read {mp}"
        msk = (msk > 127).astype(np.uint8)

        aug = self.augs(image=img, mask=msk)
        img, msk = aug["image"], aug["mask"]
        img = np.transpose(img.astype(np.float32), (2,0,1))
        msk = msk[np.newaxis, ...].astype(np.float32)
        return torch.from_numpy(img), torch.from_numpy(msk)

train_ds = SegDataset(train_pairs, size=512)
val_ds   = SegDataset(valid_pairs, size=512)

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=0)
val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0)
print("final train:", len(train_ds), " | final valid:", len(val_ds))

from matplotlib import pyplot as plt
ip, mp = valid_pairs[0]
img = cv2.cvtColor(cv2.imread(ip), cv2.COLOR_BGR2RGB)
msk = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)
plt.imshow(img); plt.title("image"); plt.axis("off"); plt.show()
plt.imshow(msk, cmap="gray"); plt.title("mask"); plt.axis("off"); plt.show()

"""##  DeepLabV3+ Training Pipeline"""

import os, torch, torch.nn as nn, numpy as np, cv2
import segmentation_models_pytorch as smp
from tqdm import tqdm

os.makedirs("/content/drive/MyDrive/pallet_project/checkpoints", exist_ok=True)
device = "cuda" if torch.cuda.is_available() else "cpu"

model = smp.DeepLabV3Plus(
    encoder_name="timm-mobilenetv3_large_100",
    encoder_weights="imagenet",
    in_channels=3,
    classes=1
).to(device)

loss_bce = nn.BCEWithLogitsLoss()
dice = smp.losses.DiceLoss(mode="binary")
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=3
)


def iou_score(pred, target, eps=1e-6):
    pred = (pred>0.5).float()
    inter = (pred*target).sum()
    union = pred.sum() + target.sum() - inter
    return ((inter+eps)/(union+eps)).item()

def visualize_batch(model, loader, k=3, epoch=0):
    model.eval()
    x,y = next(iter(loader))
    with torch.no_grad():
        p = torch.sigmoid(model(x.to(device))).cpu().numpy()
    x = x.numpy(); y = y.numpy()
    mean = np.array([0.485,0.456,0.406]); std = np.array([0.229,0.224,0.225])
    os.makedirs(f"{root}/valid/overlays", exist_ok=True)
    for i in range(min(k, x.shape[0])):
        img = (np.transpose(x[i], (1,2,0)) * std + mean)
        img = np.clip(img,0,1)
        gt  = y[i,0]
        pr  = (p[i,0] > 0.5).astype(np.uint8)
        overlay = (img*255).astype(np.uint8).copy()
        overlay[pr==1] = (overlay[pr==1]*0.5 + np.array([0,255,0])*0.5).astype(np.uint8)
        canvas = np.concatenate([
            (img*255).astype(np.uint8),
            cv2.cvtColor((gt*255).astype(np.uint8), cv2.COLOR_GRAY2BGR),
            overlay
        ], axis=1)
        cv2.imwrite(f"{root}/valid/overlays/epoch{epoch:03d}_sample{i}.png",
                    cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))

scaler = torch.cuda.amp.GradScaler(enabled=(device=="cuda"))
EPOCHS = 25
best_iou, patience_left = 0.0, 6

for epoch in range(1, EPOCHS+1):
    model.train(); tr_loss = 0.0
    for x,y in tqdm(train_dl, desc=f"epoch {epoch:02d}"):
        x,y = x.to(device), y.to(device)
        with torch.cuda.amp.autocast(enabled=(device=="cuda")):
            logits = model(x)
            loss = loss_bce(logits, y) + dice(logits, y)
        optimizer.zero_grad(); scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        tr_loss += loss.item()
    tr_loss /= max(1, len(train_dl))

    model.eval(); iou_sum=0.0; n=0
    with torch.no_grad():
        for x,y in val_dl:
            x,y = x.to(device), y.to(device)
            with torch.cuda.amp.autocast(enabled=(device=="cuda")):
                p = torch.sigmoid(model(x))
            iou_sum += iou_score(p.cpu(), y.cpu()); n += 1
    val_iou = iou_sum / max(1,n)
    scheduler.step(val_iou)
    print(f"[epoch {epoch:02d}] train_loss={tr_loss:.4f}  val_mIoU={val_iou:.4f}  best={best_iou:.4f}")

    visualize_batch(model, val_dl, k=3, epoch=epoch)

    if val_iou > best_iou + 1e-4:
        best_iou = val_iou
        torch.save(model.state_dict(), "/content/drive/MyDrive/pallet_project/checkpoints/deeplabv3p_mnv_best.pth")
        patience_left = 6
    else:
        patience_left -= 1
        if patience_left == 0:
            print("early stopping"); break

"""##  Exports my DeepLabV3 to ONNX &"""

import torch, onnxruntime as ort, numpy as np, os
model.load_state_dict(torch.load("/content/drive/MyDrive/pallet_project/checkpoints/deeplabv3p_mnv_best.pth", map_location="cpu"))
model.eval().to("cpu")

os.makedirs("/content/onnx", exist_ok=True)
dummy = torch.randn(1,3,512,512)
torch.onnx.export(
    model, dummy, "/content/onnx/deeplabv3p.onnx",
    input_names=['input'], output_names=['logits'],
    opset_version=12, dynamic_axes={'input':{0:'batch'}, 'logits':{0:'batch'}}
)
sess = ort.InferenceSession("/content/onnx/deeplabv3p.onnx", providers=['CPUExecutionProvider'])
print("ONNX test output shape:", sess.run(None, {'input': dummy.numpy()})[0].shape)

"""## Loading Detection dataset"""

import os, glob, random

det_root = "/content/drive/MyDrive/EuroPallet"  # detection splits

for sp in ["train","valid","test"]:
    n_img = len(glob.glob(f"{det_root}/{sp}/images/*"))
    n_lbl = len(glob.glob(f"{det_root}/{sp}/labels/*.txt"))
    print(f"{sp}: images={n_img}  labels(txt)={n_lbl}")

# peek one label file
sample_txt = glob.glob(f"{det_root}/train/labels/*.txt")[0]
print("sample label file:", sample_txt)
print(open(sample_txt).read().splitlines()[:5])  # each line: cls cx cy w h (normalized)

from pathlib import Path
from PIL import Image
import pandas as pd, glob, os

det_root = "/content/drive/MyDrive/EuroPallet"  # <-- your path

def make_csv(split):
    rows, missing = [], []
    imgs = sorted(glob.glob(f"{det_root}/{split}/images/*"))
    for ip in imgs:
        lp = ip.replace("/images/","/labels/").rsplit(".",1)[0] + ".txt"
        if not os.path.exists(lp):
            missing.append(ip)
            continue
        w,h = Image.open(ip).size
        with open(lp) as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) < 5:
                    continue  # skip blank/bad lines
                cls = int(float(parts[0]))
                cx, cy, bw, bh = map(float, parts[1:5])
                xmin = (cx - bw/2) * w; ymin = (cy - bh/2) * h
                xmax = (cx + bw/2) * w; ymax = (cy + bh/2) * h
                rows.append([ip, xmin, ymin, xmax, ymax, cls])
    out = f"{det_root}/annotations_{split}.csv"
    pd.DataFrame(rows, columns=["image_path","xmin","ymin","xmax","ymax","class_id"]).to_csv(out, index=False)
    print(f"{split}: wrote {out} | rows={len(rows)} | images={len(imgs)} | missing_label_files={len(missing)}")
    if missing[:5]:
        print("sample missing labels:", missing[:5])
    return out

train_csv = make_csv("train")
val_csv   = make_csv("valid")
test_csv  = make_csv("test")    # optional

import os, cv2, numpy as np, pandas as pd, torch
from torch.utils.data import Dataset, DataLoader

det_root = "/content/drive/MyDrive/EuroPallet"
IMG = 512
device = "cuda" if torch.cuda.is_available() else "cpu"

class DetDataset(Dataset):
    def __init__(self, csv_path, img_size=IMG):
        self.df = pd.read_csv(csv_path)
        self.groups = self.df.groupby('image_path')
        self.paths = list(self.groups.groups.keys())
        self.size = img_size
    def __len__(self): return len(self.paths)
    def __getitem__(self, i):
        ip = self.paths[i]
        g = self.groups.get_group(ip)  # rows for this image
        img = cv2.cvtColor(cv2.imread(ip), cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]
        img = cv2.resize(img, (self.size, self.size))
        sx, sy = self.size / w, self.size / h

        boxes, labels = [], []
        for _, r in g.iterrows():
            boxes.append([r.xmin * sx, r.ymin * sy, r.xmax * sx, r.ymax * sy])
            labels.append(int(r.class_id) + 1)   # +1 because 0 is background in EfficientDet

        boxes = np.array(boxes, np.float32) if len(boxes) else np.zeros((0,4), np.float32)
        labels = np.array(labels, np.int64) if len(labels) else np.zeros((0,), np.int64)

        img = (img / 255.0).astype(np.float32).transpose(2,0,1)
        target = {'boxes': boxes, 'labels': labels}
        return torch.from_numpy(img), target

def collate(batch):
    xs, ys = zip(*batch)
    return torch.stack(xs), list(ys)

train_csv = f"{det_root}/annotations_train.csv"
val_csv   = f"{det_root}/annotations_valid.csv"

train_dl  = DataLoader(DetDataset(train_csv), batch_size=4, shuffle=True,  num_workers=0, collate_fn=collate)
val_dl    = DataLoader(DetDataset(val_csv),   batch_size=4, shuffle=False, num_workers=0, collate_fn=collate)

# quick sanity check:
xb, yb = next(iter(train_dl))
print("batch imgs:", xb.shape)
print("first target keys:", yb[0].keys(), "boxes:", yb[0]['boxes'].shape, "labels:", yb[0]['labels'].shape)

"""## EfficientDet-D0 Object Detection Training"""

import os, numpy as np, torch
from effdet import create_model, DetBenchTrain

ckpt_dir = "/content/drive/MyDrive/pallet_project/checkpoints"
os.makedirs(ckpt_dir, exist_ok=True)
ckpt_path = f"{ckpt_dir}/effdet_d0_best.pth"

NUM_CLASSES = 1
IMG = 512
device = "cuda" if torch.cuda.is_available() else "cpu"

net = create_model(
    'tf_efficientdet_d0',
    pretrained=True,
    num_classes=NUM_CLASSES,
    image_size=(IMG, IMG)
)
model = DetBenchTrain(net, net.config).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
scaler = torch.amp.GradScaler('cuda' if device == 'cuda' else None)

def to_scalar_loss(out):
    if torch.is_tensor(out): return out
    if isinstance(out, dict):
        return out['loss'] if 'loss' in out else torch.stack([
            v if torch.is_tensor(v) else torch.as_tensor(v, dtype=torch.float32, device=device)
            for v in out.values()
        ]).sum()
    vals = [v if torch.is_tensor(v) else torch.as_tensor(v, dtype=torch.float32, device=device)
            for v in list(out)]
    return torch.stack(vals).sum()

xb, yb = next(iter(train_dl))
print("first batch labels unique (before shift):", np.unique(yb[0]['labels']))

def run_epoch(loader, train=True):
    model.train(train)
    total = 0.0
    for imgs, t_np in loader:
        imgs = imgs.to(device)
        B = imgs.shape[0]

        # dict-of-lists targets (0-based labels), plus img_scale & img_size
        bbox_list, cls_list = [], []
        for t in t_np:
            b = torch.as_tensor(t['boxes'], dtype=torch.float32, device=device)
            c = torch.as_tensor(t['labels'], dtype=torch.int64,   device=device)
            if c.numel() > 0 and c.min().item() != 0:
                c = c - c.min()   # shift to start at 0 if needed
            # (optional) clamp boxes to image bounds
            b[..., 0::2] = b[..., 0::2].clamp(0, IMG - 1)
            b[..., 1::2] = b[..., 1::2].clamp(0, IMG - 1)
            bbox_list.append(b); cls_list.append(c)

        targets = {
            'bbox': bbox_list,
            'cls':  cls_list,
            'img_scale': torch.ones(B, device=device, dtype=torch.float32),
            'img_size':  torch.full((B, 2), IMG, device=device, dtype=torch.float32)
        }

        if train:
            optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type='cuda', enabled=(device=='cuda')):
                out = model(imgs, targets)
                loss = to_scalar_loss(out)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            with torch.no_grad():
                out = model(imgs, targets)
                loss = to_scalar_loss(out)

        total += float(loss.detach().cpu().item())
    return total / max(1, len(loader))

EPOCHS = 20
best_val = float('inf')
patience, pat_left = 6, 6

for epoch in range(1, EPOCHS+1):
    tr_loss = run_epoch(train_dl, train=True)
    val_loss = run_epoch(val_dl,   train=False)
    scheduler.step(val_loss)
    print(f"[epoch {epoch:02d}] train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  best={best_val:.4f}")

    if val_loss < best_val - 1e-4:
        best_val = val_loss
        torch.save(model.state_dict(), ckpt_path)
        print("  ✔ saved best:", ckpt_path)
        pat_left = patience
    else:
        pat_left -= 1
        if pat_left == 0:
            print("Early stopping.")
            break

print("done. best val_loss:", best_val)

""" ## Exporting EfficientDet-D0 Model as TorchScript"""

import torch
from effdet import create_model

NUM_CLASSES, IMG = 1, 512
ckpt = "/content/drive/MyDrive/pallet_project/checkpoints/effdet_d0_best.pth"

# Predict bench (keeps NMS inside)
pred = create_model('tf_efficientdet_d0',
                    bench_task='predict',
                    num_classes=NUM_CLASSES,
                    image_size=(IMG, IMG),
                    pretrained=False).eval()
pred.load_state_dict(torch.load(ckpt, map_location='cpu'), strict=False)

dummy = torch.randn(1,3,IMG,IMG)
ts = torch.jit.trace(pred, dummy)               # or torch.jit.script(pred) if trace complains
ts.save("/content/onnx/effdet_d0_predict.ts")   # name is arbitrary; it's a TorchScript file
print("saved TorchScript:", "/content/onnx/effdet_d0_predict.ts")

"""## EfficientDet Inference"""

import torch, cv2, numpy as np, glob
from matplotlib import pyplot as plt

TS_PATH = "/content/onnx/effdet_d0_predict.ts"
IMG = 512

det_ts = torch.jit.load(TS_PATH, map_location="cpu").eval()

def infer_one_ts(ip, conf=0.25):
    bgr = cv2.imread(ip); h0, w0 = bgr.shape[:2]
    rgb = cv2.cvtColor(cv2.resize(bgr, (IMG, IMG)), cv2.COLOR_BGR2RGB)
    x = torch.from_numpy((rgb/255.).astype(np.float32).transpose(2,0,1))[None]
    with torch.no_grad():
        out = det_ts(x)[0].numpy()  # (N,6): x1,y1,x2,y2,score,cls (NMS already applied)
    # scale boxes back
    out[:,[0,2]] *= w0/IMG; out[:,[1,3]] *= h0/IMG
    vis = bgr.copy()
    for x1,y1,x2,y2,score,_ in out:
        if score < conf: continue
        cv2.rectangle(vis,(int(x1),int(y1)),(int(x2),int(y2)),(0,255,0),2)
        cv2.putText(vis,f"{score:.2f}",(int(x1),max(0,int(y1)-5)),
                    cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2)
    return vis, out

sample = glob.glob("/content/drive/MyDrive/EuroPallet/valid/images/*")[0]
vis, dets = infer_one_ts(sample)
plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)); plt.axis("off"); plt.show()
print("detections:", dets.shape)

"""## Metric helpers"""

import os, glob, cv2, json, time, numpy as np, onnxruntime as ort, torch
from pathlib import Path

# --- SEGMENTATION: build (image,mask) pairs for VALID split ---
def find_mask_for_image(img_path, root, split):
    p = Path(img_path); stem = p.stem
    mask_dir = Path(root)/split/"masks"
    for name in (f"{stem}.png", f"{stem}.jpg", f"{stem}.jpeg", f"{stem}_mask.png", f"{stem}_mask.jpg"):
        mp = mask_dir/name
        if mp.exists(): return str(mp)
    return None

def seg_valid_pairs(root):
    imgs = sorted(glob.glob(f"{root}/valid/images/*"))
    pairs = []
    for ip in imgs:
        mp = find_mask_for_image(ip, root, "valid")
        if mp: pairs.append((ip, mp))
    print(f"[seg] valid images={len(imgs)}  paired={len(pairs)}  missing={len(imgs)-len(pairs)}")
    return pairs

# --- DETECTION: read YOLO txt labels (normalized cx,cy,w,h) ---
def load_yolo_labels(txt_path, w, h):
    boxes, labels = [], []
    if not os.path.exists(txt_path): return np.zeros((0,4),np.float32), np.zeros((0,),np.int64)
    with open(txt_path) as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 5: continue
            cls = int(float(parts[0]))
            cx, cy, bw, bh = map(float, parts[1:5])
            x1 = (cx - bw/2) * w; y1 = (cy - bh/2) * h
            x2 = (cx + bw/2) * w; y2 = (cy + bh/2) * h
            boxes.append([x1,y1,x2,y2]); labels.append(cls)
    return np.array(boxes, np.float32), np.array(labels, np.int64)

def det_valid_images(det_root):
    imgs = sorted(glob.glob(f"{det_root}/valid/images/*"))
    print(f"[det] valid images={len(imgs)}")
    return imgs

# --- metrics ---
def iou_binary(pred, gt):
    inter = np.logical_and(pred, gt).sum()
    union = np.logical_or(pred, gt).sum()
    if union == 0: return 1.0
    return inter / union

def dice_binary(pred, gt, eps=1e-6):
    inter = np.logical_and(pred, gt).sum()
    return (2*inter + eps) / (pred.sum() + gt.sum() + eps)

def box_iou_np(a, b):
    # a: [N,4], b: [M,4]  -> IoU [N,M]
    if a.size==0 or b.size==0: return np.zeros((len(a), len(b)), np.float32)
    ax1,ay1,ax2,ay2 = a[:,0],a[:,1],a[:,2],a[:,3]
    bx1,by1,bx2,by2 = b[:,0],b[:,1],b[:,2],b[:,3]
    inter_x1 = np.maximum(ax1[:,None], bx1[None])
    inter_y1 = np.maximum(ay1[:,None], by1[None])
    inter_x2 = np.minimum(ax2[:,None], bx2[None])
    inter_y2 = np.minimum(ay2[:,None], by2[None])
    inter_w = np.clip(inter_x2 - inter_x1, 0, None)
    inter_h = np.clip(inter_y2 - inter_y1, 0, None)
    inter = inter_w * inter_h
    area_a = (ax2-ax1)*(ay2-ay1)
    area_b = (bx2-bx1)*(by2-by1)
    union = area_a[:,None] + area_b[None] - inter
    return np.where(union>0, inter/union, 0.0).astype(np.float32)

def ap_at_05(preds, gts):
    """
    preds: list of arrays per image [[x1,y1,x2,y2,score], ...]
    gts:   list of arrays per image [[x1,y1,x2,y2], ...]
    Computes AP@0.5 IoU (single class).
    """
    all_scores, all_matches = [], []
    total_gts = 0
    for p, g in zip(preds, gts):
        total_gts += len(g)
        if len(p)==0:
            continue
        ious = box_iou_np(p[:, :4], g)
        max_iou_idx = ious.argmax(axis=1)
        max_iou = ious[np.arange(len(p)), max_iou_idx] if len(g) else np.zeros((len(p),), np.float32)
        # Greedy: a GT can be matched once
        matched_gt = set()
        matches = []
        order = np.argsort(-p[:,4])  # sort by score desc
        for idx in order:
            i = idx
            if len(g)==0:
                matches.append(0)
                continue
            gt_j = max_iou_idx[i]
            if max_iou[i] >= 0.5 and gt_j not in matched_gt:
                matched_gt.add(gt_j); matches.append(1)
            else:
                matches.append(0)
            all_scores.append(p[i,4])
        all_matches.extend(matches)

    if len(all_matches)==0:
        return 0.0
    # sort by score again to form PR curve
    order = np.argsort(-np.array(all_scores))
    tp = np.cumsum(np.array(all_matches)[order])
    fp = np.cumsum(1 - np.array(all_matches)[order])
    recall = tp / max(1, total_gts)
    precision = tp / np.maximum(1, tp + fp)
    # 11-point interpolation (PASCAL VOC style)
    ap = 0.0
    for r in np.linspace(0,1,11):
        prec_at_r = precision[recall >= r].max() if np.any(recall >= r) else 0.0
        ap += prec_at_r / 11.0
    return float(ap)

import os
os.makedirs("/content/onnx", exist_ok=True)

import torch, segmentation_models_pytorch as smp, onnxruntime as ort

CKPT = "/content/drive/MyDrive/pallet_project/checkpoints/deeplabv3p_mnv_best.pth"  # adjust if named differently
IMG = 512

model = smp.DeepLabV3Plus(
    encoder_name="timm-mobilenetv3_large_100",
    encoder_weights=None,
    in_channels=3,
    classes=1
).eval()

state = torch.load(CKPT, map_location="cpu")
model.load_state_dict(state["state_dict"] if "state_dict" in state else state, strict=False)

dummy = torch.randn(1,3,IMG,IMG)
torch.onnx.export(
    model, dummy, "/content/onnx/deeplabv3p.onnx",
    input_names=["input"], output_names=["logits"],
    opset_version=12, dynamic_axes={"input":{0:"batch"}, "logits":{0:"batch"}}
)
ort.InferenceSession("/content/onnx/deeplabv3p.onnx", providers=["CPUExecutionProvider"])
print(" Exported DeepLabv3+ to /content/onnx/deeplabv3p.onnx")

import torch
from effdet import create_model

NUM_CLASSES, IMG = 1, 512
CKPT = "/content/drive/MyDrive/pallet_project/checkpoints/effdet_d0_best.pth"

pred = create_model('tf_efficientdet_d0', bench_task='predict',
                    num_classes=NUM_CLASSES, image_size=(IMG,IMG),
                    pretrained=False).eval()

pred.load_state_dict(torch.load(CKPT, map_location="cpu"), strict=False)
dummy = torch.randn(1,3,IMG,IMG)
ts = torch.jit.trace(pred, dummy)
ts.save("/content/onnx/effdet_d0_predict.ts")
print("✅ Saved TorchScript EfficientDet at /content/onnx/effdet_d0_predict.ts")

"""##Sorting out right file paths"""

SEG_ONNX_MINE    = "/content/onnx/deeplabv3p.onnx"
SEG_ONNX_TEAMMATE = "/content/drive/MyDrive/pallet_project/shared_models/ground_seg.onnx"
DET_TS_MINE      = "/content/onnx/effdet_d0_predict.ts"
DET_ONNX_TEAMMATE = "/content/drive/MyDrive/pallet_project/shared_models/best.onnx"

import os
for n,p in {"SEG_ONNX_MINE":SEG_ONNX_YOURS,"SEG_ONNX_TEAMMATE":SEG_ONNX_TEAMMATE,
            "DET_TS_MINE":DET_TS_YOURS,"DET_ONNX_TEAMMATE":DET_ONNX_TEAMMATE}.items():
    print(f"{n:20s} exists? {os.path.exists(p)}  ->  {p}")

"""## Functions for metrics"""

import numpy as np, cv2, os, glob, time, torch
from pathlib import Path

# seg pairs
def find_mask_for_image(img_path, root, split):
    p = Path(img_path); stem = p.stem
    for name in (f"{stem}.png", f"{stem}.jpg", f"{stem}.jpeg", f"{stem}_mask.png"):
        mp = Path(root)/split/"masks"/name
        if mp.exists(): return str(mp)
    return None

def seg_valid_pairs(root):
    imgs = sorted(glob.glob(f"{root}/valid/images/*"))
    pairs = []
    for ip in imgs:
        mp = find_mask_for_image(ip, root, "valid")
        if mp: pairs.append((ip, mp))
    print(f"[seg] valid images={len(imgs)}  paired={len(pairs)}")
    return pairs

# det gt loader
def load_yolo_labels(txt_path, w, h):
    boxes=[]
    if not os.path.exists(txt_path): return np.zeros((0,4),np.float32)
    with open(txt_path) as f:
        for line in f:
            parts=line.strip().split()
            if len(parts)<5: continue
            cx,cy,bw,bh = map(float, parts[1:5])
            x1=(cx-bw/2)*w; y1=(cy-bh/2)*h; x2=(cx+bw/2)*w; y2=(cy+bh/2)*h
            boxes.append([x1,y1,x2,y2])
    return np.array(boxes,np.float32)

def det_valid_images(root):
    imgs = sorted(glob.glob(f"{root}/valid/images/*"))
    print(f"[det] valid images={len(imgs)}")
    return imgs

# metrics
def iou_binary(pred, gt):
    inter = np.logical_and(pred, gt).sum()
    union = np.logical_or(pred, gt).sum()
    return 1.0 if union==0 else inter/union

def dice_binary(pred, gt, eps=1e-6):
    inter = np.logical_and(pred, gt).sum()
    return (2*inter+eps)/(pred.sum()+gt.sum()+eps)

def box_iou_np(a,b):
    if a.size==0 or b.size==0: return np.zeros((len(a), len(b)), np.float32)
    ax1,ay1,ax2,ay2 = a[:,0],a[:,1],a[:,2],a[:,3]
    bx1,by1,bx2,by2 = b[:,0],b[:,1],b[:,2],b[:,3]
    inter_x1 = np.maximum(ax1[:,None], bx1[None])
    inter_y1 = np.maximum(ay1[:,None], by1[None])
    inter_x2 = np.minimum(ax2[:,None], bx2[None])
    inter_y2 = np.minimum(ay2[:,None], by2[None])
    inter_w = np.clip(inter_x2 - inter_x1, 0, None)
    inter_h = np.clip(inter_y2 - inter_y1, 0, None)
    inter = inter_w * inter_h
    area_a = (ax2-ax1)*(ay2-ay1)
    area_b = (bx2-bx1)*(by2-by1)
    union = area_a[:,None] + area_b[None] - inter
    return np.where(union>0, inter/union, 0.0).astype(np.float32)

def ap_at_05(preds, gts):
    all_scores, all_matches, total_gts = [], [], 0
    for p,g in zip(preds,gts):
        total_gts += len(g)
        if len(p)==0: continue
        ious = box_iou_np(p[:,:4], g)
        order = np.argsort(-p[:,4])
        taken=set()
        for i in order:
            if len(g)==0:
                all_scores.append(p[i,4]); all_matches.append(0); continue
            j = ious[i].argmax()
            iou = ious[i,j]
            if iou>=0.5 and j not in taken:
                taken.add(j); all_matches.append(1)
            else:
                all_matches.append(0)
            all_scores.append(p[i,4])
    if not all_scores: return 0.0
    order = np.argsort(-np.array(all_scores))
    tp = np.cumsum(np.array(all_matches)[order])
    fp = np.cumsum(1 - np.array(all_matches)[order])
    recall = tp / max(1,total_gts)
    precision = tp / np.maximum(1, tp+fp)
    ap=0.0
    for r in np.linspace(0,1,11):
        ap += (precision[recall>=r].max() if np.any(recall>=r) else 0.0)/11.0
    return float(ap)

"""## Loading models for comparison"""

# 5.1 – load models
seg_you = ort.InferenceSession(SEG_ONNX_MINE,    providers=["CPUExecutionProvider"])
seg_tm  = ort.InferenceSession(SEG_ONNX_TEAMMATE, providers=["CPUExecutionProvider"])

det_ts  = torch.jit.load(DET_TS_MINE, map_location="cpu").eval()
det_tm  = ort.InferenceSession(DET_ONNX_TEAMMATE, providers=["CPUExecutionProvider"])

# 5.2 – segmentation metrics (fixed for different layouts)

def seg_eval(sess, pairs, img_size, layout="NCHW", thr=0.5):
    """
    layout: "NCHW"  -> [1, 3, H, W]
            "NHWC"  -> [1, H, W, 3]
    img_size: the H, W that the model expects
    """
    t0 = time.perf_counter()
    ious, dices, accs = [], [], []

    inp_name = sess.get_inputs()[0].name

    for ip, mp in pairs:
        # load and resize image
        bgr = cv2.imread(ip)
        rgb = cv2.cvtColor(cv2.resize(bgr, (img_size, img_size)), cv2.COLOR_BGR2RGB)

        if layout == "NCHW":
            x = (rgb / 255.0).astype(np.float32).transpose(2, 0, 1)[None]  # [1,3,H,W]
        else:  # "NHWC"
            x = (rgb / 255.0).astype(np.float32)[None]                      # [1,H,W,3]

        # run model
        logits = sess.run(None, {inp_name: x})[0]   # shape [1,1,H,W] or [1,H,W,1] etc.
        # squeeze channel & batch
        logit = logits.squeeze()
        prob = 1 / (1 + np.exp(-logit))
        pred = (prob > thr).astype(np.uint8)

        # load and resize GT mask
        gt_raw = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)
        gt = cv2.resize(gt_raw, (img_size, img_size), interpolation=cv2.INTER_NEAREST)
        gt = (gt > 127).astype(np.uint8)

        # metrics
        inter = np.logical_and(pred, gt).sum()
        union = np.logical_or(pred, gt).sum()
        iou = 1.0 if union == 0 else inter / union
        dice = (2 * inter + 1e-6) / (pred.sum() + gt.sum() + 1e-6)
        acc = (pred == gt).mean()

        ious.append(iou); dices.append(dice); accs.append(acc)

    dt = time.perf_counter() - t0
    return {
        "mIoU": float(np.mean(ious)),
        "Dice": float(np.mean(dices)),
        "PixelAcc": float(np.mean(accs)),
        "Latency_ms": (dt / len(pairs)) * 1000.0,
        "FPS": float(len(pairs) / dt)
    }

# Build the validation pairs once
pairs = seg_valid_pairs(SEG_ROOT)

# Your DeepLabv3+  -> NCHW, 512x512
seg_you_metrics = seg_eval(seg_you, pairs, img_size=512, layout="NCHW")

# Teammate U-Net   -> NHWC, 256x256
seg_tm_metrics = seg_eval(seg_tm, pairs, img_size=256, layout="NHWC")

print("Seg (Me):      ", seg_you_metrics)
print("Seg (Teammate):", seg_tm_metrics)

# 5.3 – detection metrics
def det_predict_yours(ip, conf=0.25):
    bgr = cv2.imread(ip); h0,w0 = bgr.shape[:2]
    rgb = cv2.cvtColor(cv2.resize(bgr,(IMG,IMG)), cv2.COLOR_BGR2RGB)
    x = torch.from_numpy((rgb/255.).astype(np.float32).transpose(2,0,1))[None]
    with torch.no_grad():
        out = det_ts(x)[0].numpy()   # (100,6): x1,y1,x2,y2,score,cls
    keep = out[:,4]>=conf
    out = out[keep]
    out[:,[0,2]] *= w0/IMG; out[:,[1,3]] *= h0/IMG
    return out[:, :5]  # xyxy+score

def det_predict_tm(ip, conf=0.25):
    bgr = cv2.imread(ip); h0,w0 = bgr.shape[:2]
    rgb = cv2.cvtColor(cv2.resize(bgr,(IMG,IMG)), cv2.COLOR_BGR2RGB)
    x = (rgb/255.).astype(np.float32).transpose(2,0,1)[None]
    out = det_tm.run(None, {det_tm.get_inputs()[0].name: x})[0][0]
    # if yolov8 format [N, 4 + C]: convert cxcywh + class scores
    if out.shape[1] > 6:
        boxes = out[:, :4]; scores = out[:, 4:]
        cls = scores.argmax(1); sc = scores.max(1)
        cx,cy,w,h = boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]
        x1=(cx-w/2)*w0/IMG; y1=(cy-h/2)*h0/IMG; x2=(cx+w/2)*w0/IMG; y2=(cy+h/2)*h0/IMG
        out = np.stack([x1,y1,x2,y2,sc], axis=1)
    else:
        # already xyxy+score on 0..IMG grid
        out[:,[0,2]] *= w0/IMG; out[:,[1,3]] *= h0/IMG
        out = out[:, :5]
    return out[out[:,4]>=conf]

val_imgs = det_valid_images(DET_ROOT)

def eval_detector(predict_fn, imgs, conf=0.25):
    preds=[]; gts=[]; t0=time.perf_counter()
    for ip in imgs:
        det = predict_fn(ip, conf=conf)
        preds.append(det)
        w,h = cv2.imread(ip).shape[1], cv2.imread(ip).shape[0]
        lp = ip.replace("/images/","/labels/").rsplit(".",1)[0] + ".txt"
        gts.append(load_yolo_labels(lp, w, h))
    dt=time.perf_counter()-t0
    return {"mAP@0.5": ap_at_05(preds,gts), "Latency_ms": (dt/len(imgs))*1000.0, "FPS": float(len(imgs)/dt)}

det_you_metrics = eval_detector(det_predict_yours, val_imgs, conf=0.25)
det_tm_metrics  = eval_detector(det_predict_tm,   val_imgs, conf=0.25)
print("Det (Me):", det_you_metrics)
print("Det (Teammate):", det_tm_metrics)

# 5.3 – detection metrics
def det_predict_yours(ip, conf=0.25):
    bgr = cv2.imread(ip); h0,w0 = bgr.shape[:2]
    rgb = cv2.cvtColor(cv2.resize(bgr,(IMG,IMG)), cv2.COLOR_BGR2RGB)
    x = torch.from_numpy((rgb/255.).astype(np.float32).transpose(2,0,1))[None]
    with torch.no_grad():
        out = det_ts(x)[0].numpy()   # (100,6): x1,y1,x2,y2,score,cls
    keep = out[:,4]>=conf
    out = out[keep]
    out[:,[0,2]] *= w0/IMG; out[:,[1,3]] *= h0/IMG
    return out[:, :5]  # xyxy+score

def det_predict_tm(ip, conf=0.25):
    bgr = cv2.imread(ip); h0,w0 = bgr.shape[:2]
    rgb = cv2.cvtColor(cv2.resize(bgr,(IMG,IMG)), cv2.COLOR_BGR2RGB)
    x = (rgb/255.).astype(np.float32).transpose(2,0,1)[None]
    out = det_tm.run(None, {det_tm.get_inputs()[0].name: x})[0][0]
    # if yolov8 format [N, 4 + C]: convert cxcywh + class scores
    if out.shape[1] > 6:
        boxes = out[:, :4]; scores = out[:, 4:]
        cls = scores.argmax(1); sc = scores.max(1)
        cx,cy,w,h = boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]
        x1=(cx-w/2)*w0/IMG; y1=(cy-h/2)*h0/IMG; x2=(cx+w/2)*w0/IMG; y2=(cy+h/2)*h0/IMG
        out = np.stack([x1,y1,x2,y2,sc], axis=1)
    else:
        # already xyxy+score on 0..IMG grid
        out[:,[0,2]] *= w0/IMG; out[:,[1,3]] *= h0/IMG
        out = out[:, :5]
    return out[out[:,4]>=conf]

val_imgs = det_valid_images(DET_ROOT)

def eval_detector(predict_fn, imgs, conf=0.25):
    preds=[]; gts=[]; t0=time.perf_counter()
    for ip in imgs:
        det = predict_fn(ip, conf=conf)
        preds.append(det)
        w,h = cv2.imread(ip).shape[1], cv2.imread(ip).shape[0]
        lp = ip.replace("/images/","/labels/").rsplit(".",1)[0] + ".txt"
        gts.append(load_yolo_labels(lp, w, h))
    dt=time.perf_counter()-t0
    return {"mAP@0.5": ap_at_05(preds,gts), "Latency_ms": (dt/len(imgs))*1000.0, "FPS": float(len(imgs)/dt)}

det_you_metrics = eval_detector(det_predict_yours, val_imgs, conf=0.25)
det_tm_metrics  = eval_detector(det_predict_tm,   val_imgs, conf=0.25)
print("Det (You):", det_you_metrics)
print("Det (Teammate):", det_tm_metrics)

"""## YOLOv8 ONNX helper"""

import numpy as np, cv2

# Helper: get YOLO input H,W and layout from the ONNX model
inp_det = det_tm.get_inputs()[0]
inp_name_tm = inp_det.name
inp_shape_tm = inp_det.shape  # usually [1, 3, 640, 640] for YOLOv8

print("Teammate det input name:", inp_name_tm)
print("Teammate det input shape:", inp_shape_tm)

# Try to infer layout and spatial size
if len(inp_shape_tm) == 4:
    b0, c_or_h, h_or_w, maybe_w = inp_shape_tm
    # assume NCHW if second dim is small (3)
    if c_or_h == 3:
        DET_TM_LAYOUT = "NCHW"
        DET_TM_H = h_or_w
        DET_TM_W = maybe_w
    else:
        DET_TM_LAYOUT = "NHWC"
        DET_TM_H = c_or_h
        DET_TM_W = h_or_w
else:
    DET_TM_LAYOUT = "NCHW"
    DET_TM_H = DET_TM_W = 640  # fallback

print("Using layout:", DET_TM_LAYOUT, "H,W:", DET_TM_H, DET_TM_W)

def det_predict_tm(ip, conf=0.25):
    bgr = cv2.imread(ip)
    h0, w0 = bgr.shape[:2]

    # resize to what ONNX expects
    resized = cv2.resize(bgr, (DET_TM_W, DET_TM_H))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    img = (rgb / 255.0).astype(np.float32)

    if DET_TM_LAYOUT == "NCHW":
        x = img.transpose(2, 0, 1)[None]    # [1,3,H,W]
    else:  # NHWC
        x = img[None]                       # [1,H,W,3]

    out = det_tm.run(None, {inp_name_tm: x})[0]
    out = np.squeeze(out)


    if out.ndim == 2 and out.shape[1] >= 6:
        # Case A: [N, x1, y1, x2, y2, score, (class)]
        boxes = out[:, :4]
        scores = out[:, 4]
        # ignore class for now (single class)
    elif out.ndim == 3 and out.shape[1] == 6:
        # Case B: [1, 6, N] -> transpose to [N, 6]
        out = out.transpose(2, 1, 0).reshape(-1, 6)
        boxes = out[:, :4]
        scores = out[:, 4]
    else:
        # Fallback: no predictions
        return np.zeros((0,5), np.float32)

    # scale from network coordinates back to original image size
    # assume boxes are already in xyxy relative to [0, DET_TM_W/H]
    # If they are absolute on the resized image, this still works.
    x1 = boxes[:,0] * (w0 / DET_TM_W)
    y1 = boxes[:,1] * (h0 / DET_TM_H)
    x2 = boxes[:,2] * (w0 / DET_TM_W)
    y2 = boxes[:,3] * (h0 / DET_TM_H)

    dets = np.stack([x1, y1, x2, y2, scores], axis=1)
    dets = dets[dets[:,4] >= conf]
    return dets.astype(np.float32)

"""## Model Evaluation on Detection Validation Set"""

val_imgs = det_valid_images(DET_ROOT)

det_you_metrics = eval_detector(det_predict_yours, val_imgs, conf=0.25)
det_tm_metrics  = eval_detector(det_predict_tm,   val_imgs, conf=0.25)

print("Det (Me):", det_you_metrics)
print("Det (Teammate):", det_tm_metrics)

"""## Metrics Comparison table"""

import pandas as pd, os
def size_mb(p): return round(os.path.getsize(p)/1e6,2) if os.path.exists(p) else None
rows = [
  {"Task":"Segmentation","Model":"DeepLabv3+ (mine)", **seg_you_metrics, "Size(MB)": size_mb(SEG_ONNX_YOURS)},
  {"Task":"Segmentation","Model":"U-Net (teammate)",   **seg_tm_metrics,  "Size(MB)": size_mb(SEG_ONNX_TEAMMATE)},
  {"Task":"Detection","Model":"EfficientDet-D0 (mine)", **det_you_metrics, "Size(MB)": size_mb(DET_TS_YOURS)},
  {"Task":"Detection","Model":"YOLOv8 (teammate)",       **det_tm_metrics,  "Size(MB)": size_mb(DET_ONNX_TEAMMATE)},
]
pd.DataFrame(rows)

import glob

print("Searching for your checkpoints...")

seg_ckpts = glob.glob("/content/drive/**/*deeplab*.*", recursive=True)
det_ckpts = glob.glob("/content/drive/**/*effdet*.*", recursive=True)

print("\nSegmentation checkpoints found:")
print(seg_ckpts)

print("\nDetection checkpoints found:")
print(det_ckpts)

